{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://medium.com/@csakash03/evaluating-rag-with-llamaindex-3f74a35c53fa\n",
    "\n",
    "#another link for eval\n",
    "#https://docs.llamaindex.ai/en/stable/examples/node_postprocessor/MetadataReplacementDemo.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import nest_asyncio\n",
    "from llama_index.core import VectorStoreIndex, ServiceContext, SimpleDirectoryReader, Document\n",
    "from llama_index.llms.llama_cpp import LlamaCPP\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core.node_parser import SentenceWindowNodeParser, SentenceSplitter\n",
    "from llama_index.core.indices.postprocessor import SentenceTransformerRerank, MetadataReplacementPostProcessor\n",
    "from llama_index.core.evaluation import DatasetGenerator, CorrectnessEvaluator, RelevancyEvaluator, FaithfulnessEvaluator\n",
    "from llama_index.core.evaluation.eval_utils import get_responses, get_results_df\n",
    "from llama_index.core.evaluation import BatchEvalRunner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from /Users/yasaminabbaszadegan/Library/Caches/llama_index/models/mistral-7b-instruct-v0.2.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.22 MiB\n",
      "ggml_backend_metal_buffer_from_ptr: allocated buffer, size =  4095.08 MiB, (16775.92 / 21845.34)\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
      "llm_load_tensors:      Metal buffer size =  4095.07 MiB\n",
      ".................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 3900\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Max\n",
      "ggml_metal_init: picking default device: Apple M2 Max\n",
      "ggml_metal_init: default.metallib not found, loading from source\n",
      "ggml_metal_init: GGML_METAL_PATH_RESOURCES = nil\n",
      "ggml_metal_init: loading '/Users/yasaminabbaszadegan/.virtualenvs/venv/lib/python3.10/site-packages/llama_cpp/ggml-metal.metal'\n",
      "ggml_metal_init: GPU name:   Apple M2 Max\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 22906.50 MB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =   487.50 MiB, (17264.05 / 21845.34)\n",
      "llama_kv_cache_init:      Metal KV buffer size =   487.50 MiB\n",
      "llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB\n",
      "llama_new_context_with_model:        CPU input buffer size   =    16.65 MiB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =   283.38 MiB, (17547.42 / 21845.34)\n",
      "llama_new_context_with_model:      Metal compute buffer size =   283.37 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =     8.00 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 2\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.add_bos_token': 'true', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.model': 'llama', 'llama.attention.head_count_kv': '8', 'llama.context_length': '32768', 'llama.attention.head_count': '32', 'llama.rope.freq_base': '1000000.000000', 'llama.rope.dimension_count': '128', 'general.file_type': '15', 'llama.feed_forward_length': '14336', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'general.architecture': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'general.name': 'mistralai_mistral-7b-instruct-v0.2'}\n",
      "Guessed chat format: mistral-instruct\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize LLAMA model and components\n",
    "llm = LlamaCPP(\n",
    "    model_url='https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q4_K_M.gguf',\n",
    "    temperature=1,\n",
    "    model_kwargs={\"n_gpu_layers\": -1},\n",
    "    verbose=True\n",
    ")\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "rerank = SentenceTransformerRerank(top_n=2, model=\"BAAI/bge-reranker-base\")\n",
    "postproc = MetadataReplacementPostProcessor(target_metadata_key=\"window\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fr/9w484qsj00v863fh8tnryj780000gn/T/ipykernel_85463/1899550090.py:13: DeprecationWarning: Call to deprecated class DatasetGenerator. (Deprecated in favor of `RagDatasetGenerator` which should be used instead.)\n",
      "  dataset_generator = DatasetGenerator(\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      "llama_print_timings:        load time =    5912.42 ms\n",
      "llama_print_timings:      sample time =       9.63 ms /   107 runs   (    0.09 ms per token, 11106.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6414.72 ms /   767 tokens (    8.36 ms per token,   119.57 tokens per second)\n",
      "llama_print_timings:        eval time =    2031.34 ms /   106 runs   (   19.16 ms per token,    52.18 tokens per second)\n",
      "llama_print_timings:       total time =    8588.95 ms /   873 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    5912.42 ms\n",
      "llama_print_timings:      sample time =      13.11 ms /   109 runs   (    0.12 ms per token,  8316.17 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1213.73 ms /   636 tokens (    1.91 ms per token,   524.01 tokens per second)\n",
      "llama_print_timings:        eval time =    2078.65 ms /   108 runs   (   19.25 ms per token,    51.96 tokens per second)\n",
      "llama_print_timings:       total time =    3497.60 ms /   744 tokens\n",
      "100%|██████████| 2/2 [00:12<00:00,  6.05s/it]\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    5912.42 ms\n",
      "llama_print_timings:      sample time =       4.32 ms /    34 runs   (    0.13 ms per token,  7866.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =     147.38 ms /    34 tokens (    4.33 ms per token,   230.69 tokens per second)\n",
      "llama_print_timings:        eval time =     630.26 ms /    33 runs   (   19.10 ms per token,    52.36 tokens per second)\n",
      "llama_print_timings:       total time =     844.01 ms /    67 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    5912.42 ms\n",
      "llama_print_timings:      sample time =       7.18 ms /    54 runs   (    0.13 ms per token,  7525.08 tokens per second)\n",
      "llama_print_timings: prompt eval time =      99.27 ms /    24 tokens (    4.14 ms per token,   241.77 tokens per second)\n",
      "llama_print_timings:        eval time =    1001.13 ms /    53 runs   (   18.89 ms per token,    52.94 tokens per second)\n",
      "llama_print_timings:       total time =    1204.49 ms /    77 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    5912.42 ms\n",
      "llama_print_timings:      sample time =       3.84 ms /    46 runs   (    0.08 ms per token, 11988.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =     145.67 ms /    36 tokens (    4.05 ms per token,   247.13 tokens per second)\n",
      "llama_print_timings:        eval time =     842.65 ms /    45 runs   (   18.73 ms per token,    53.40 tokens per second)\n",
      "llama_print_timings:       total time =    1042.32 ms /    81 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    5912.42 ms\n",
      "llama_print_timings:      sample time =       5.29 ms /    45 runs   (    0.12 ms per token,  8508.22 tokens per second)\n",
      "llama_print_timings: prompt eval time =     100.40 ms /    28 tokens (    3.59 ms per token,   278.89 tokens per second)\n",
      "llama_print_timings:        eval time =     834.91 ms /    44 runs   (   18.98 ms per token,    52.70 tokens per second)\n",
      "llama_print_timings:       total time =    1013.06 ms /    72 tokens\n",
      "100%|██████████| 4/4 [00:04<00:00,  1.03s/it]\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    5912.42 ms\n",
      "llama_print_timings:      sample time =       8.06 ms /    68 runs   (    0.12 ms per token,  8431.49 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1365.11 ms /   724 tokens (    1.89 ms per token,   530.36 tokens per second)\n",
      "llama_print_timings:        eval time =    1293.32 ms /    67 runs   (   19.30 ms per token,    51.80 tokens per second)\n",
      "llama_print_timings:       total time =    2785.51 ms /   791 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    5912.42 ms\n",
      "llama_print_timings:      sample time =      12.48 ms /   103 runs   (    0.12 ms per token,  8254.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =      96.38 ms /    32 tokens (    3.01 ms per token,   332.01 tokens per second)\n",
      "llama_print_timings:        eval time =    1966.34 ms /   102 runs   (   19.28 ms per token,    51.87 tokens per second)\n",
      "llama_print_timings:       total time =    2254.18 ms /   134 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    5912.42 ms\n",
      "llama_print_timings:      sample time =      10.37 ms /    81 runs   (    0.13 ms per token,  7814.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =      98.87 ms /    22 tokens (    4.49 ms per token,   222.53 tokens per second)\n",
      "llama_print_timings:        eval time =    1546.52 ms /    80 runs   (   19.33 ms per token,    51.73 tokens per second)\n",
      "llama_print_timings:       total time =    1800.72 ms /   102 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    5912.42 ms\n",
      "llama_print_timings:      sample time =       9.58 ms /    90 runs   (    0.11 ms per token,  9392.61 tokens per second)\n",
      "llama_print_timings: prompt eval time =      99.45 ms /    23 tokens (    4.32 ms per token,   231.27 tokens per second)\n",
      "llama_print_timings:        eval time =    1724.03 ms /    89 runs   (   19.37 ms per token,    51.62 tokens per second)\n",
      "llama_print_timings:       total time =    1965.19 ms /   112 tokens\n",
      "100%|██████████| 4/4 [00:08<00:00,  2.21s/it]\n",
      "/Users/yasaminabbaszadegan/.virtualenvs/venv/lib/python3.10/site-packages/llama_index/core/evaluation/dataset_generation.py:309: DeprecationWarning: Call to deprecated class QueryResponseDataset. (Deprecated in favor of `LabelledRagDataset` which should be used instead.)\n",
      "  return QueryResponseDataset(queries=queries, responses=responses_dict)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create node parser and postprocessors\n",
    "node_parser = SentenceWindowNodeParser.from_defaults(\n",
    "    window_size=5,\n",
    "    window_metadata_key=\"window\",\n",
    "    original_text_metadata_key=\"original_text\"\n",
    ")\n",
    "postprocessors = [postproc, rerank]\n",
    "\n",
    "# Load documents and build sentence index\n",
    "docs = SimpleDirectoryReader(input_files=['./temp/4583673.pdf']).load_data()\n",
    "documents = Document(text=\"\\n\\n\".join([doc.text for doc in docs]))\n",
    "sentence_context = ServiceContext.from_defaults(llm=llm, embed_model=embed_model, node_parser=node_parser)\n",
    "sentence_index = VectorStoreIndex.from_documents([documents], service_context=sentence_context)\n",
    "\n",
    "# Setup evaluation dataset\n",
    "num_nodes_eval = 2\n",
    "base_nodes = SentenceSplitter().get_nodes_from_documents(docs)\n",
    "sample_eval_nodes = random.sample(base_nodes, num_nodes_eval)\n",
    "dataset_generator = DatasetGenerator(sample_eval_nodes, llm=llm, show_progress=True, num_questions_per_chunk=4)\n",
    "eval_dataset = await dataset_generator.agenerate_dataset_from_nodes()\n",
    "\n",
    "# Save evaluation dataset\n",
    "eval_dataset.save_json(\"./eval_dataset/qr_dataset.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'queries': {'87ce6719-5d76-4acb-a099-860077ccb20e': 'Who are the property owners of PLAN 254E PT LOTS 23 & 24, as indicated by the document?',\n",
       "  '861f10f6-ea84-4465-b65b-842fab947ccd': 'What is the last date for appealing this decision to the Ontario Municipal Board, as stated in the document?',\n",
       "  'a365db2b-ca96-4d00-85d6-831f1073cb8a': 'What is the filing fee required to appeal this decision to the Ontario Municipal Board, as stated in the document?',\n",
       "  '7074585b-f607-4881-b467-10756f8fdb84': 'What is the legal description and community of the property address 91 Leuty Ave, according to the document?',\n",
       "  'c1c5b476-5edc-4361-a9b5-0275ae3bc085': 'What are the two requirements mentioned in the document that would not be met by the proposed rear deck addition?',\n",
       "  '6908c8a9-0c41-41a0-b650-ccc7f4a825a4': 'Which two sections of By-law 438 -86 are referenced in the document in regards to parking and access requirements?',\n",
       "  'e6d660b9-2790-4a8f-9071-feaf698960b4': 'What are the two dimensions of a building that were exceeded by the proposed rear deck addition according to the Zoning By-law?',\n",
       "  'fc0599f0-5a8d-44f2-ab44-8266fe77982d': 'In what year did houses built before become subject to certain restrictions in terms of additions?'},\n",
       " 'responses': {'87ce6719-5d76-4acb-a099-860077ccb20e': ' MICHAEL SUTTON and MELANIE COURTOIS are the property owners of PLAN 254E PT LOTS 23 & 24, as indicated by the document.',\n",
       "  '861f10f6-ea84-4465-b65b-842fab947ccd': ' The last date for appealing this decision to the Ontario Municipal Board, as stated in the document, is Tuesday, May 10, 2016.',\n",
       "  'a365db2b-ca96-4d00-85d6-831f1073cb8a': 'The filing fee required to appeal this decision to the Ontario Municipal Board is $125.00, as stated in the document. An additional reduced fee of $25.00 is required for each connected appeal filed by the same appellant.',\n",
       "  '7074585b-f607-4881-b467-10756f8fdb84': ' The legal description of the property address 91 Leuty Ave, according to the document, is PLAN 254E PT LOTS 23 & 24 and the community is Toronto.',\n",
       "  'c1c5b476-5edc-4361-a9b5-0275ae3bc085': '1) The minimum required width of an access driveway leading to a parking facility will be eliminated by the rear deck (as per Section 4(5)(I)(I) of By-law 438-86). 2) No parking space will be provided (as per Section 4(4)(b) of By-law 438-86).',\n",
       "  '6908c8a9-0c41-41a0-b650-ccc7f4a825a4': '1. Section 4(5)(I)(I) which discusses the minimum required width of an access driveway leading to a parking facility and a turning radius at the end of a right-of-way, and 2. Section 4(4)(b) which requires at least one parking space to be provided.',\n",
       "  'e6d660b9-2790-4a8f-9071-feaf698960b4': '1) Minimum required width of an access driveway leading to a parking facility - eliminated by the rear deck. Required width is 2.6 m, and no turning radius at the end of the right-of-way is provided.\\n   2) Depth of the residential building - the altered semi-detached dwelling will have a building depth of 19.23 m, while the maximum depth according to the zoning by-law is 17.0 m.',\n",
       "  'fc0599f0-5a8d-44f2-ab44-8266fe77982d': '1953. According to the context information, additions to the rear of houses erected before October 15, 1953, or to converted houses, are subject to certain restrictions regarding the residential gross floor area and the depth of the building as stated in Section 6(3) Part VI 1(I) and 1(V) of By-law 438-86 respectively.'}}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset.dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset.save_json(\"./eval_dataset/qr_dataset.json\")\n",
    "# optional\n",
    "# eval_dataset = QueryResponseDataset.from_json(\"data/ipcc_eval_qr_dataset.json\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Setup evaluators\n",
    "evaluator_c = CorrectnessEvaluator(llm=llm)\n",
    "evaluator_r = RelevancyEvaluator(llm=llm)\n",
    "evaluator_f = FaithfulnessEvaluator(llm=llm)\n",
    "\n",
    "# Setup query engines\n",
    "base_query_engine = sentence_index.as_query_engine(similarity_top_k=6)\n",
    "query_engine = sentence_index.as_query_engine(\n",
    "    similarity_top_k=6,\n",
    "    node_postprocessors=[\n",
    "        MetadataReplacementPostProcessor(target_metadata_key=\"window\"),\n",
    "        SentenceTransformerRerank(top_n=2, model=\"BAAI/bge-reranker-base\")\n",
    "    ]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:00<?, ?it/s]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    5912.42 ms\n",
      "llama_print_timings:      sample time =      25.67 ms /   254 runs   (    0.10 ms per token,  9895.98 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2231.75 ms /   298 tokens (    7.49 ms per token,   133.53 tokens per second)\n",
      "llama_print_timings:        eval time =    4714.45 ms /   253 runs   (   18.63 ms per token,    53.66 tokens per second)\n",
      "llama_print_timings:       total time =    7367.71 ms /   551 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    5912.42 ms\n",
      "llama_print_timings:      sample time =      11.82 ms /    96 runs   (    0.12 ms per token,  8119.08 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1531.18 ms /   826 tokens (    1.85 ms per token,   539.45 tokens per second)\n",
      "llama_print_timings:        eval time =    1854.58 ms /    95 runs   (   19.52 ms per token,    51.22 tokens per second)\n",
      "llama_print_timings:       total time =    3551.01 ms /   921 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    5912.42 ms\n",
      "llama_print_timings:      sample time =       5.45 ms /    40 runs   (    0.14 ms per token,  7344.84 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1861.50 ms /  1010 tokens (    1.84 ms per token,   542.57 tokens per second)\n",
      "llama_print_timings:        eval time =     785.30 ms /    39 runs   (   20.14 ms per token,    49.66 tokens per second)\n",
      "llama_print_timings:       total time =    2726.21 ms /  1049 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    5912.42 ms\n",
      "llama_print_timings:      sample time =      25.93 ms /   254 runs   (    0.10 ms per token,  9795.98 tokens per second)\n",
      "llama_print_timings: prompt eval time =     588.04 ms /   302 tokens (    1.95 ms per token,   513.57 tokens per second)\n",
      "llama_print_timings:        eval time =    4727.44 ms /   253 runs   (   18.69 ms per token,    53.52 tokens per second)\n",
      "llama_print_timings:       total time =    5690.36 ms /   555 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    5912.42 ms\n",
      "llama_print_timings:      sample time =       3.38 ms /    40 runs   (    0.08 ms per token, 11827.32 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1758.54 ms /   932 tokens (    1.89 ms per token,   529.98 tokens per second)\n",
      "llama_print_timings:        eval time =     773.38 ms /    39 runs   (   19.83 ms per token,    50.43 tokens per second)\n",
      "llama_print_timings:       total time =    2586.82 ms /   971 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    5912.42 ms\n",
      "llama_print_timings:      sample time =       2.25 ms /    27 runs   (    0.08 ms per token, 12010.68 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1739.80 ms /   950 tokens (    1.83 ms per token,   546.04 tokens per second)\n",
      "llama_print_timings:        eval time =     516.50 ms /    26 runs   (   19.87 ms per token,    50.34 tokens per second)\n",
      "llama_print_timings:       total time =    2292.90 ms /   976 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    5912.42 ms\n",
      "llama_print_timings:      sample time =       7.95 ms /    59 runs   (    0.13 ms per token,  7425.12 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1123.01 ms /   586 tokens (    1.92 ms per token,   521.81 tokens per second)\n",
      "llama_print_timings:        eval time =    1096.09 ms /    58 runs   (   18.90 ms per token,    52.92 tokens per second)\n",
      "llama_print_timings:       total time =    2323.77 ms /   644 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    5912.42 ms\n",
      "llama_print_timings:      sample time =       7.62 ms /    77 runs   (    0.10 ms per token, 10098.36 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1533.44 ms /   810 tokens (    1.89 ms per token,   528.23 tokens per second)\n",
      "llama_print_timings:        eval time =    1499.18 ms /    76 runs   (   19.73 ms per token,    50.69 tokens per second)\n",
      "llama_print_timings:       total time =    3157.70 ms /   886 tokens\n",
      "100%|██████████| 8/8 [00:30<00:00,  3.79s/it]\n",
      "  0%|          | 0/8 [00:00<?, ?it/s]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    5912.42 ms\n",
      "llama_print_timings:      sample time =      13.38 ms /   117 runs   (    0.11 ms per token,  8745.05 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2132.02 ms /  1125 tokens (    1.90 ms per token,   527.67 tokens per second)\n",
      "llama_print_timings:        eval time =    2372.12 ms /   116 runs   (   20.45 ms per token,    48.90 tokens per second)\n",
      "llama_print_timings:       total time =    4689.73 ms /  1241 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    5912.42 ms\n",
      "llama_print_timings:      sample time =       4.02 ms /    39 runs   (    0.10 ms per token,  9691.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1209.67 ms /   634 tokens (    1.91 ms per token,   524.11 tokens per second)\n",
      "llama_print_timings:        eval time =     772.77 ms /    38 runs   (   20.34 ms per token,    49.17 tokens per second)\n",
      "llama_print_timings:       total time =    2036.29 ms /   672 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    5912.42 ms\n",
      "llama_print_timings:      sample time =       8.27 ms /    55 runs   (    0.15 ms per token,  6647.33 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2083.71 ms /  1099 tokens (    1.90 ms per token,   527.42 tokens per second)\n",
      "llama_print_timings:        eval time =    1103.33 ms /    54 runs   (   20.43 ms per token,    48.94 tokens per second)\n",
      "llama_print_timings:       total time =    3310.81 ms /  1153 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    5912.42 ms\n",
      "llama_print_timings:      sample time =      17.02 ms /   130 runs   (    0.13 ms per token,  7639.42 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1524.52 ms /   817 tokens (    1.87 ms per token,   535.91 tokens per second)\n",
      "llama_print_timings:        eval time =    2541.20 ms /   129 runs   (   19.70 ms per token,    50.76 tokens per second)\n",
      "llama_print_timings:       total time =    4306.68 ms /   946 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    5912.42 ms\n",
      "llama_print_timings:      sample time =       8.95 ms /    90 runs   (    0.10 ms per token, 10056.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1237.21 ms /   655 tokens (    1.89 ms per token,   529.42 tokens per second)\n",
      "llama_print_timings:        eval time =    1718.01 ms /    89 runs   (   19.30 ms per token,    51.80 tokens per second)\n",
      "llama_print_timings:       total time =    3079.82 ms /   744 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    5912.42 ms\n",
      "llama_print_timings:      sample time =       4.29 ms /    43 runs   (    0.10 ms per token, 10011.64 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2414.37 ms /  1297 tokens (    1.86 ms per token,   537.20 tokens per second)\n",
      "llama_print_timings:        eval time =     872.26 ms /    42 runs   (   20.77 ms per token,    48.15 tokens per second)\n",
      "llama_print_timings:       total time =    3348.50 ms /  1339 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    5912.42 ms\n",
      "llama_print_timings:      sample time =       4.15 ms /    34 runs   (    0.12 ms per token,  8202.65 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2449.50 ms /  1295 tokens (    1.89 ms per token,   528.68 tokens per second)\n",
      "llama_print_timings:        eval time =     687.81 ms /    33 runs   (   20.84 ms per token,    47.98 tokens per second)\n",
      "llama_print_timings:       total time =    3204.83 ms /  1328 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    5912.42 ms\n",
      "llama_print_timings:      sample time =       6.69 ms /    60 runs   (    0.11 ms per token,  8972.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2235.10 ms /  1153 tokens (    1.94 ms per token,   515.86 tokens per second)\n",
      "llama_print_timings:        eval time =    1211.83 ms /    59 runs   (   20.54 ms per token,    48.69 tokens per second)\n",
      "llama_print_timings:       total time =    3542.42 ms /  1212 tokens\n",
      "100%|██████████| 8/8 [00:29<00:00,  3.69s/it]\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.evaluation.eval_utils import (\n",
    "    get_responses,\n",
    "    get_results_df,\n",
    ")\n",
    "from llama_index.core.evaluation import BatchEvalRunner\n",
    "\n",
    "max_samples = 30\n",
    "\n",
    "eval_qs = eval_dataset.questions\n",
    "ref_response_strs = [r for (_, r) in eval_dataset.qr_pairs]\n",
    "\n",
    "# resetup base query engine and sentence window query engine\n",
    "# base query engine\n",
    "node_parser = SentenceSplitter()\n",
    "documents = SimpleDirectoryReader(input_files=['./temp/4583673.pdf']).load_data()\n",
    "documents = Document(text=\"\\n\\n\".join([doc.text for doc in documents]))\n",
    "base_index = VectorStoreIndex.from_documents([documents], service_context=sentence_context)\n",
    "base_query_engine = base_index.as_query_engine(similarity_top_k=6)\n",
    "# sentence window query engine\n",
    "\n",
    "query_engine = sentence_index.as_query_engine(\n",
    "    similarity_top_k=6,\n",
    "    # the target key defaults to `window` to match the node_parser's default\n",
    "    node_postprocessors=[\n",
    "        MetadataReplacementPostProcessor(target_metadata_key=\"window\"),SentenceTransformerRerank(top_n=2, model=\"BAAI/bge-reranker-base\")\n",
    "    ],\n",
    ")\n",
    "import numpy as np\n",
    "\n",
    "base_pred_responses = get_responses(\n",
    "    eval_qs[:max_samples], base_query_engine, show_progress=True\n",
    ")\n",
    "pred_responses = get_responses(\n",
    "    eval_qs[:max_samples], query_engine, show_progress=True\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pred_response_strs = [str(p) for p in pred_responses]\n",
    "base_pred_response_strs = [str(p) for p in base_pred_responses]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/24 [00:00<?, ?it/s]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    5912.42 ms\n",
      "llama_print_timings:      sample time =       4.78 ms /    36 runs   (    0.13 ms per token,  7532.96 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2521.12 ms /  1296 tokens (    1.95 ms per token,   514.06 tokens per second)\n",
      "llama_print_timings:        eval time =     732.70 ms /    35 runs   (   20.93 ms per token,    47.77 tokens per second)\n",
      "llama_print_timings:       total time =    3332.59 ms /  1331 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    5912.42 ms\n",
      "llama_print_timings:      sample time =      13.91 ms /    79 runs   (    0.18 ms per token,  5681.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1795.39 ms /   934 tokens (    1.92 ms per token,   520.22 tokens per second)\n",
      "llama_print_timings:        eval time =    1580.93 ms /    78 runs   (   20.27 ms per token,    49.34 tokens per second)\n",
      "llama_print_timings:       total time =    3587.21 ms /  1012 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    5912.42 ms\n",
      "llama_print_timings:      sample time =       0.90 ms /     3 runs   (    0.30 ms per token,  3325.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2759.44 ms /  1470 tokens (    1.88 ms per token,   532.72 tokens per second)\n",
      "llama_print_timings:        eval time =      43.91 ms /     2 runs   (   21.95 ms per token,    45.55 tokens per second)\n",
      "llama_print_timings:       total time =    2829.06 ms /  1472 tokens\n",
      "  4%|▍         | 1/24 [00:09<03:44,  9.78s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    5912.42 ms\n",
      "llama_print_timings:      sample time =       4.11 ms /    42 runs   (    0.10 ms per token, 10218.98 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1048.93 ms /   537 tokens (    1.95 ms per token,   511.95 tokens per second)\n",
      "llama_print_timings:        eval time =     775.01 ms /    41 runs   (   18.90 ms per token,    52.90 tokens per second)\n",
      "llama_print_timings:       total time =    1878.71 ms /   578 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    5912.42 ms\n",
      "llama_print_timings:      sample time =       8.20 ms /    83 runs   (    0.10 ms per token, 10126.89 tokens per second)\n",
      "llama_print_timings: prompt eval time =     310.83 ms /   134 tokens (    2.32 ms per token,   431.10 tokens per second)\n",
      "llama_print_timings:        eval time =    1528.15 ms /    82 runs   (   18.64 ms per token,    53.66 tokens per second)\n",
      "llama_print_timings:       total time =    1941.44 ms /   216 tokens\n",
      " 12%|█▎        | 3/24 [00:13<01:23,  3.95s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    5912.42 ms\n",
      "llama_print_timings:      sample time =      18.56 ms /   184 runs   (    0.10 ms per token,  9914.33 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1805.45 ms /   976 tokens (    1.85 ms per token,   540.59 tokens per second)\n",
      "llama_print_timings:        eval time =    3683.70 ms /   183 runs   (   20.13 ms per token,    49.68 tokens per second)\n",
      "llama_print_timings:       total time =    5745.96 ms /  1159 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    5912.42 ms\n",
      "llama_print_timings:      sample time =       5.69 ms /    58 runs   (    0.10 ms per token, 10186.16 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1508.53 ms /   803 tokens (    1.88 ms per token,   532.31 tokens per second)\n",
      "llama_print_timings:        eval time =    1114.75 ms /    57 runs   (   19.56 ms per token,    51.13 tokens per second)\n",
      "llama_print_timings:       total time =    2697.72 ms /   860 tokens\n",
      " 21%|██        | 5/24 [00:22<01:17,  4.10s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    5912.42 ms\n",
      "llama_print_timings:      sample time =       6.35 ms /    49 runs   (    0.13 ms per token,  7712.89 tokens per second)\n",
      "llama_print_timings: prompt eval time =     857.32 ms /   476 tokens (    1.80 ms per token,   555.22 tokens per second)\n",
      "llama_print_timings:        eval time =     908.26 ms /    48 runs   (   18.92 ms per token,    52.85 tokens per second)\n",
      "llama_print_timings:       total time =    1849.34 ms /   524 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    5912.42 ms\n",
      "llama_print_timings:      sample time =      12.66 ms /    90 runs   (    0.14 ms per token,  7110.69 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2388.91 ms /  1267 tokens (    1.89 ms per token,   530.37 tokens per second)\n",
      "llama_print_timings:        eval time =    1851.08 ms /    89 runs   (   20.80 ms per token,    48.08 tokens per second)\n",
      "llama_print_timings:       total time =    4438.12 ms /  1356 tokens\n",
      " 29%|██▉       | 7/24 [00:28<01:02,  3.70s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    5912.42 ms\n",
      "llama_print_timings:      sample time =       4.70 ms /    39 runs   (    0.12 ms per token,  8303.17 tokens per second)\n",
      "llama_print_timings: prompt eval time =     877.45 ms /   452 tokens (    1.94 ms per token,   515.13 tokens per second)\n",
      "llama_print_timings:        eval time =     711.00 ms /    38 runs   (   18.71 ms per token,    53.45 tokens per second)\n",
      "llama_print_timings:       total time =    1648.26 ms /   490 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    5912.42 ms\n",
      "llama_print_timings:      sample time =       6.05 ms /    56 runs   (    0.11 ms per token,  9260.79 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2307.52 ms /  1222 tokens (    1.89 ms per token,   529.57 tokens per second)\n",
      "llama_print_timings:        eval time =    1133.84 ms /    55 runs   (   20.62 ms per token,    48.51 tokens per second)\n",
      "llama_print_timings:       total time =    3530.39 ms /  1277 tokens\n",
      " 38%|███▊      | 9/24 [00:33<00:49,  3.28s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    5912.42 ms\n",
      "llama_print_timings:      sample time =       5.78 ms /    52 runs   (    0.11 ms per token,  8993.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2684.11 ms /  1447 tokens (    1.85 ms per token,   539.10 tokens per second)\n",
      "llama_print_timings:        eval time =    1079.29 ms /    51 runs   (   21.16 ms per token,    47.25 tokens per second)\n",
      "llama_print_timings:       total time =    3850.27 ms /  1498 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    5912.42 ms\n",
      "llama_print_timings:      sample time =       8.94 ms /    69 runs   (    0.13 ms per token,  7716.39 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2584.96 ms /  1395 tokens (    1.85 ms per token,   539.66 tokens per second)\n",
      "llama_print_timings:        eval time =    1431.00 ms /    68 runs   (   21.04 ms per token,    47.52 tokens per second)\n",
      "llama_print_timings:       total time =    4164.50 ms /  1463 tokens\n",
      " 46%|████▌     | 11/24 [00:41<00:46,  3.54s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    5912.42 ms\n",
      "llama_print_timings:      sample time =       2.17 ms /    18 runs   (    0.12 ms per token,  8302.58 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3026.22 ms /  1573 tokens (    1.92 ms per token,   519.79 tokens per second)\n",
      "llama_print_timings:        eval time =     363.00 ms /    17 runs   (   21.35 ms per token,    46.83 tokens per second)\n",
      "llama_print_timings:       total time =    3431.19 ms /  1590 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    5912.42 ms\n",
      "llama_print_timings:      sample time =       3.56 ms /    22 runs   (    0.16 ms per token,  6184.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =     694.03 ms /   379 tokens (    1.83 ms per token,   546.08 tokens per second)\n",
      "llama_print_timings:        eval time =     394.83 ms /    21 runs   (   18.80 ms per token,    53.19 tokens per second)\n",
      "llama_print_timings:       total time =    1137.81 ms /   400 tokens\n",
      " 54%|█████▍    | 13/24 [00:46<00:34,  3.12s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    5912.42 ms\n",
      "llama_print_timings:      sample time =       3.91 ms /    31 runs   (    0.13 ms per token,  7924.34 tokens per second)\n",
      "llama_print_timings: prompt eval time =     264.20 ms /   125 tokens (    2.11 ms per token,   473.13 tokens per second)\n",
      "llama_print_timings:        eval time =     559.00 ms /    30 runs   (   18.63 ms per token,    53.67 tokens per second)\n",
      "llama_print_timings:       total time =     876.03 ms /   155 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    5912.42 ms\n",
      "llama_print_timings:      sample time =       3.96 ms /    22 runs   (    0.18 ms per token,  5558.36 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2943.98 ms /  1564 tokens (    1.88 ms per token,   531.25 tokens per second)\n",
      "llama_print_timings:        eval time =     454.28 ms /    21 runs   (   21.63 ms per token,    46.23 tokens per second)\n",
      "llama_print_timings:       total time =    3461.80 ms /  1585 tokens\n",
      " 62%|██████▎   | 15/24 [00:50<00:25,  2.82s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    5912.42 ms\n",
      "llama_print_timings:      sample time =       9.97 ms /    80 runs   (    0.12 ms per token,  8023.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2267.71 ms /  1209 tokens (    1.88 ms per token,   533.14 tokens per second)\n",
      "llama_print_timings:        eval time =    1625.52 ms /    79 runs   (   20.58 ms per token,    48.60 tokens per second)\n",
      "llama_print_timings:       total time =    4053.33 ms /  1288 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    5912.42 ms\n",
      "llama_print_timings:      sample time =       4.22 ms /    21 runs   (    0.20 ms per token,  4981.02 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2489.72 ms /  1314 tokens (    1.89 ms per token,   527.77 tokens per second)\n",
      "llama_print_timings:        eval time =     427.78 ms /    20 runs   (   21.39 ms per token,    46.75 tokens per second)\n",
      "llama_print_timings:       total time =    2990.90 ms /  1334 tokens\n",
      " 71%|███████   | 17/24 [00:57<00:21,  3.04s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    5912.42 ms\n",
      "llama_print_timings:      sample time =       0.75 ms /     3 runs   (    0.25 ms per token,  3994.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2591.98 ms /  1388 tokens (    1.87 ms per token,   535.50 tokens per second)\n",
      "llama_print_timings:        eval time =      43.41 ms /     2 runs   (   21.71 ms per token,    46.07 tokens per second)\n",
      "llama_print_timings:       total time =    2654.76 ms /  1390 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    5912.42 ms\n",
      "llama_print_timings:      sample time =       4.86 ms /    22 runs   (    0.22 ms per token,  4531.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =     801.98 ms /   421 tokens (    1.90 ms per token,   524.95 tokens per second)\n",
      "llama_print_timings:        eval time =     402.75 ms /    21 runs   (   19.18 ms per token,    52.14 tokens per second)\n",
      "llama_print_timings:       total time =    1274.72 ms /   442 tokens\n",
      " 79%|███████▉  | 19/24 [01:01<00:13,  2.71s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    5912.42 ms\n",
      "llama_print_timings:      sample time =       5.39 ms /    32 runs   (    0.17 ms per token,  5932.52 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2209.71 ms /  1184 tokens (    1.87 ms per token,   535.82 tokens per second)\n",
      "llama_print_timings:        eval time =     642.02 ms /    31 runs   (   20.71 ms per token,    48.29 tokens per second)\n",
      "llama_print_timings:       total time =    2937.26 ms /  1215 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    5912.42 ms\n",
      "llama_print_timings:      sample time =       9.61 ms /    58 runs   (    0.17 ms per token,  6036.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1027.22 ms /   528 tokens (    1.95 ms per token,   514.01 tokens per second)\n",
      "llama_print_timings:        eval time =    1085.42 ms /    57 runs   (   19.04 ms per token,    52.51 tokens per second)\n",
      "llama_print_timings:       total time =    2249.24 ms /   585 tokens\n",
      " 88%|████████▊ | 21/24 [01:06<00:08,  2.68s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    5912.42 ms\n",
      "llama_print_timings:      sample time =       0.61 ms /     3 runs   (    0.20 ms per token,  4909.98 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2611.17 ms /  1392 tokens (    1.88 ms per token,   533.09 tokens per second)\n",
      "llama_print_timings:        eval time =      43.09 ms /     2 runs   (   21.54 ms per token,    46.42 tokens per second)\n",
      "llama_print_timings:       total time =    2672.06 ms /  1394 tokens\n",
      "100%|██████████| 24/24 [01:09<00:00,  2.89s/it]\n",
      "  0%|          | 0/24 [00:00<?, ?it/s]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    5912.42 ms\n",
      "llama_print_timings:      sample time =       8.65 ms /    65 runs   (    0.13 ms per token,  7512.71 tokens per second)\n",
      "llama_print_timings: prompt eval time =     881.52 ms /   457 tokens (    1.93 ms per token,   518.43 tokens per second)\n",
      "llama_print_timings:        eval time =    1211.38 ms /    64 runs   (   18.93 ms per token,    52.83 tokens per second)\n",
      "llama_print_timings:       total time =    2214.11 ms /   521 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    5912.42 ms\n",
      "llama_print_timings:      sample time =      11.21 ms /    93 runs   (    0.12 ms per token,  8293.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1812.66 ms /   976 tokens (    1.86 ms per token,   538.43 tokens per second)\n",
      "llama_print_timings:        eval time =    1839.72 ms /    92 runs   (   20.00 ms per token,    50.01 tokens per second)\n",
      "llama_print_timings:       total time =    3814.47 ms /  1068 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    5912.42 ms\n",
      "llama_print_timings:      sample time =       5.58 ms /    35 runs   (    0.16 ms per token,  6275.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2137.07 ms /  1150 tokens (    1.86 ms per token,   538.12 tokens per second)\n",
      "llama_print_timings:        eval time =     698.17 ms /    34 runs   (   20.53 ms per token,    48.70 tokens per second)\n",
      "llama_print_timings:       total time =    2912.76 ms /  1184 tokens\n",
      "  4%|▍         | 1/24 [00:08<03:26,  8.97s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    5912.42 ms\n",
      "llama_print_timings:      sample time =       6.32 ms /    42 runs   (    0.15 ms per token,  6641.37 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1891.31 ms /  1000 tokens (    1.89 ms per token,   528.73 tokens per second)\n",
      "llama_print_timings:        eval time =     856.54 ms /    41 runs   (   20.89 ms per token,    47.87 tokens per second)\n",
      "llama_print_timings:       total time =    2841.10 ms /  1041 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    5912.42 ms\n",
      "llama_print_timings:      sample time =       9.30 ms /    83 runs   (    0.11 ms per token,  8920.89 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1192.91 ms /   610 tokens (    1.96 ms per token,   511.36 tokens per second)\n",
      "llama_print_timings:        eval time =    1573.53 ms /    82 runs   (   19.19 ms per token,    52.11 tokens per second)\n",
      "llama_print_timings:       total time =    2896.70 ms /   692 tokens\n",
      " 12%|█▎        | 3/24 [00:14<01:33,  4.46s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    5912.42 ms\n",
      "llama_print_timings:      sample time =      16.68 ms /   109 runs   (    0.15 ms per token,  6532.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1013.37 ms /   516 tokens (    1.96 ms per token,   509.19 tokens per second)\n",
      "llama_print_timings:        eval time =    2042.89 ms /   108 runs   (   18.92 ms per token,    52.87 tokens per second)\n",
      "llama_print_timings:       total time =    3297.78 ms /   624 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    5912.42 ms\n",
      "llama_print_timings:      sample time =       4.43 ms /    41 runs   (    0.11 ms per token,  9261.35 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1457.30 ms /   783 tokens (    1.86 ms per token,   537.30 tokens per second)\n",
      "llama_print_timings:        eval time =     779.65 ms /    40 runs   (   19.49 ms per token,    51.31 tokens per second)\n",
      "llama_print_timings:       total time =    2295.91 ms /   823 tokens\n",
      " 21%|██        | 5/24 [00:20<01:08,  3.61s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    5912.42 ms\n",
      "llama_print_timings:      sample time =       6.50 ms /    53 runs   (    0.12 ms per token,  8157.61 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1206.09 ms /   640 tokens (    1.88 ms per token,   530.64 tokens per second)\n",
      "llama_print_timings:        eval time =    1000.04 ms /    52 runs   (   19.23 ms per token,    52.00 tokens per second)\n",
      "llama_print_timings:       total time =    2302.26 ms /   692 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    5912.42 ms\n",
      "llama_print_timings:      sample time =       5.27 ms /    39 runs   (    0.14 ms per token,  7396.17 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1479.20 ms /   784 tokens (    1.89 ms per token,   530.02 tokens per second)\n",
      "llama_print_timings:        eval time =     741.71 ms /    38 runs   (   19.52 ms per token,    51.23 tokens per second)\n",
      "llama_print_timings:       total time =    2298.10 ms /   822 tokens\n",
      " 29%|██▉       | 7/24 [00:24<00:51,  3.06s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    5912.42 ms\n",
      "llama_print_timings:      sample time =       2.34 ms /    18 runs   (    0.13 ms per token,  7685.74 tokens per second)\n",
      "llama_print_timings: prompt eval time =     690.26 ms /   372 tokens (    1.86 ms per token,   538.93 tokens per second)\n",
      "llama_print_timings:        eval time =     311.01 ms /    17 runs   (   18.29 ms per token,    54.66 tokens per second)\n",
      "llama_print_timings:       total time =    1033.46 ms /   389 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    5912.42 ms\n",
      "llama_print_timings:      sample time =       6.42 ms /    47 runs   (    0.14 ms per token,  7318.59 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1127.81 ms /   604 tokens (    1.87 ms per token,   535.55 tokens per second)\n",
      "llama_print_timings:        eval time =     882.53 ms /    46 runs   (   19.19 ms per token,    52.12 tokens per second)\n",
      "llama_print_timings:       total time =    2106.60 ms /   650 tokens\n",
      " 38%|███▊      | 9/24 [00:28<00:37,  2.50s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    5912.42 ms\n",
      "llama_print_timings:      sample time =      11.31 ms /    91 runs   (    0.12 ms per token,  8048.11 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1237.94 ms /   646 tokens (    1.92 ms per token,   521.84 tokens per second)\n",
      "llama_print_timings:        eval time =    1733.24 ms /    90 runs   (   19.26 ms per token,    51.93 tokens per second)\n",
      "llama_print_timings:       total time =    3135.01 ms /   736 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    5912.42 ms\n",
      "llama_print_timings:      sample time =       3.64 ms /    30 runs   (    0.12 ms per token,  8248.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1981.73 ms /  1026 tokens (    1.93 ms per token,   517.73 tokens per second)\n",
      "llama_print_timings:        eval time =     584.62 ms /    29 runs   (   20.16 ms per token,    49.61 tokens per second)\n",
      "llama_print_timings:       total time =    2621.94 ms /  1055 tokens\n",
      " 46%|████▌     | 11/24 [00:33<00:34,  2.63s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    5912.42 ms\n",
      "llama_print_timings:      sample time =      32.05 ms /   256 runs   (    0.13 ms per token,  7987.52 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2253.81 ms /  1211 tokens (    1.86 ms per token,   537.31 tokens per second)\n",
      "llama_print_timings:        eval time =    5305.92 ms /   255 runs   (   20.81 ms per token,    48.06 tokens per second)\n",
      "llama_print_timings:       total time =    8051.52 ms /  1466 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    5912.42 ms\n",
      "llama_print_timings:      sample time =       2.64 ms /    18 runs   (    0.15 ms per token,  6820.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =     763.62 ms /   408 tokens (    1.87 ms per token,   534.30 tokens per second)\n",
      "llama_print_timings:        eval time =     314.86 ms /    17 runs   (   18.52 ms per token,    53.99 tokens per second)\n",
      "llama_print_timings:       total time =    1114.91 ms /   425 tokens\n",
      " 54%|█████▍    | 13/24 [00:43<00:36,  3.29s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    5912.42 ms\n",
      "llama_print_timings:      sample time =       0.30 ms /     3 runs   (    0.10 ms per token, 10067.11 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2255.42 ms /  1204 tokens (    1.87 ms per token,   533.83 tokens per second)\n",
      "llama_print_timings:        eval time =      40.71 ms /     2 runs   (   20.35 ms per token,    49.13 tokens per second)\n",
      "llama_print_timings:       total time =    2307.22 ms /  1206 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    5912.42 ms\n",
      "llama_print_timings:      sample time =       5.43 ms /    43 runs   (    0.13 ms per token,  7923.35 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1990.29 ms /  1031 tokens (    1.93 ms per token,   518.02 tokens per second)\n",
      "llama_print_timings:        eval time =     843.26 ms /    42 runs   (   20.08 ms per token,    49.81 tokens per second)\n",
      "llama_print_timings:       total time =    2919.45 ms /  1073 tokens\n",
      " 62%|██████▎   | 15/24 [00:48<00:27,  3.07s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    5912.42 ms\n",
      "llama_print_timings:      sample time =       3.48 ms /    24 runs   (    0.15 ms per token,  6894.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =     761.22 ms /   398 tokens (    1.91 ms per token,   522.85 tokens per second)\n",
      "llama_print_timings:        eval time =     430.79 ms /    23 runs   (   18.73 ms per token,    53.39 tokens per second)\n",
      "llama_print_timings:       total time =    1240.24 ms /   421 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    5912.42 ms\n",
      "llama_print_timings:      sample time =       7.39 ms /    67 runs   (    0.11 ms per token,  9061.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =     363.06 ms /   167 tokens (    2.17 ms per token,   459.98 tokens per second)\n",
      "llama_print_timings:        eval time =    1234.28 ms /    66 runs   (   18.70 ms per token,    53.47 tokens per second)\n",
      "llama_print_timings:       total time =    1701.62 ms /   233 tokens\n",
      " 71%|███████   | 17/24 [00:51<00:17,  2.57s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    5912.42 ms\n",
      "llama_print_timings:      sample time =       9.49 ms /    66 runs   (    0.14 ms per token,  6951.76 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1753.23 ms /   941 tokens (    1.86 ms per token,   536.73 tokens per second)\n",
      "llama_print_timings:        eval time =    1299.48 ms /    65 runs   (   19.99 ms per token,    50.02 tokens per second)\n",
      "llama_print_timings:       total time =    3195.90 ms /  1006 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    5912.42 ms\n",
      "llama_print_timings:      sample time =       6.85 ms /    51 runs   (    0.13 ms per token,  7440.91 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2099.58 ms /  1120 tokens (    1.87 ms per token,   533.44 tokens per second)\n",
      "llama_print_timings:        eval time =    1021.10 ms /    50 runs   (   20.42 ms per token,    48.97 tokens per second)\n",
      "llama_print_timings:       total time =    3220.59 ms /  1170 tokens\n",
      " 79%|███████▉  | 19/24 [00:57<00:13,  2.77s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    5912.42 ms\n",
      "llama_print_timings:      sample time =       4.66 ms /    46 runs   (    0.10 ms per token,  9862.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1295.79 ms /   699 tokens (    1.85 ms per token,   539.44 tokens per second)\n",
      "llama_print_timings:        eval time =     867.12 ms /    45 runs   (   19.27 ms per token,    51.90 tokens per second)\n",
      "llama_print_timings:       total time =    2226.52 ms /   744 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    5912.42 ms\n",
      "llama_print_timings:      sample time =       6.22 ms /    50 runs   (    0.12 ms per token,  8038.59 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1652.71 ms /   882 tokens (    1.87 ms per token,   533.67 tokens per second)\n",
      "llama_print_timings:        eval time =     960.48 ms /    49 runs   (   19.60 ms per token,    51.02 tokens per second)\n",
      "llama_print_timings:       total time =    2703.11 ms /   931 tokens\n",
      " 88%|████████▊ | 21/24 [01:02<00:08,  2.68s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    5912.42 ms\n",
      "llama_print_timings:      sample time =       5.05 ms /    44 runs   (    0.11 ms per token,  8712.87 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2107.36 ms /  1104 tokens (    1.91 ms per token,   523.88 tokens per second)\n",
      "llama_print_timings:        eval time =     862.88 ms /    43 runs   (   20.07 ms per token,    49.83 tokens per second)\n",
      "llama_print_timings:       total time =    3050.12 ms /  1147 tokens\n",
      "100%|██████████| 24/24 [01:05<00:00,  2.74s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>names</th>\n",
       "      <th>correctness</th>\n",
       "      <th>relevancy</th>\n",
       "      <th>faithfulness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence Window Retriever</td>\n",
       "      <td>5.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Base Retriever</td>\n",
       "      <td>4.875</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.875</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       names  correctness  relevancy  faithfulness\n",
       "0  Sentence Window Retriever        5.000      1.000         0.750\n",
       "1             Base Retriever        4.875      0.875         0.875"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get responses for evaluation\n",
    "nest_asyncio.apply()\n",
    "\n",
    "max_samples = 30\n",
    "eval_qs = eval_dataset.questions\n",
    "ref_response_strs = [r for (_, r) in eval_dataset.qr_pairs]\n",
    "base_pred_responses = get_responses(eval_qs[:max_samples], base_query_engine, show_progress=True)\n",
    "pred_responses = get_responses(eval_qs[:max_samples], query_engine, show_progress=True)\n",
    "\n",
    "# Evaluate responses\n",
    "evaluator_dict = {\n",
    "    \"correctness\": evaluator_c, #Correctness: Compares the generated answer against the ground-truth answer.\n",
    "    \"faithfulness\": evaluator_f, #Faithfulness: Evaluates whether a response is faithful to the contexts (label-free)\n",
    "    \"relevancy\": evaluator_r, #to measure if the response + source nodes match the query.This is useful for measuring if the query was actually answered by the response\n",
    "}\n",
    "batch_runner = BatchEvalRunner(evaluator_dict, workers=2, show_progress=True)\n",
    "eval_results = await batch_runner.aevaluate_responses(\n",
    "    queries=eval_qs[:max_samples],\n",
    "    responses=pred_responses[:max_samples],\n",
    "    reference=ref_response_strs[:max_samples]\n",
    ")\n",
    "base_eval_results = await batch_runner.aevaluate_responses(\n",
    "    queries=eval_qs[:max_samples],\n",
    "    responses=base_pred_responses[:max_samples],\n",
    "    reference=ref_response_strs[:max_samples]\n",
    ")\n",
    "\n",
    "# Display results\n",
    "results_df = get_results_df(\n",
    "    [eval_results, base_eval_results],\n",
    "    [\"Sentence Window Retriever\", \"Base Retriever\"],\n",
    "    [\"correctness\", \"relevancy\", \"faithfulness\"]\n",
    ")\n",
    "display(results_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
